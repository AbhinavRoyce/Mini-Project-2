{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU1SCVj6dyi1"
   },
   "source": [
    "\n",
    "\n",
    "<h1><center><font size=10> Introduction to LLMs and GenAI</center></font></h1>\n",
    "<h1><center>Mini Project 2: Word2vec and GloVe</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjncuDf2qugI"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9VSf2D_F5iU"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dense-medicaid"
   },
   "source": [
    "In today’s fast-paced e-commerce landscape, customer reviews significantly influence product perception and buying decisions. Businesses must actively monitor customer sentiment to extract insights and maintain a competitive edge. Ignoring negative feedback can lead to serious issues, such as:\n",
    "\n",
    "Customer Churn: Unresolved complaints drive loyal customers away, reducing retention and future revenue.\n",
    "\n",
    "Reputation Damage: Persistent negative sentiment can erode brand trust and deter new buyers.\n",
    "\n",
    "Financial Loss: Declining sales and shifting customer preference toward competitors directly impact profitability.\n",
    "\n",
    "Actively tracking and addressing customer sentiment is essential for sustained growth and brand strength."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ6qUJ3EqxW2"
   },
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkjASpFDeR4m"
   },
   "source": [
    "A growing e-commerce platform specializing in electronic gadgets collects customer feedback from product reviews, surveys, and social media. With a 200% increase in their customer base over three years and a recent 25% spike in feedback volume, their manual review process is no longer sustainable.\n",
    "\n",
    "To address this, the company aims to implement an AI-driven solution to automatically classify customer sentiments (positive, negative, or neutral).\n",
    "\n",
    "As a Data Scientist, your task is to analyze the provided customer reviews—along with their labeled sentiments—and build a predictive model for sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_YwOCVbLB4I"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saFx1pbT_zTP"
   },
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xA7JZQ9_2SO"
   },
   "source": [
    "- **Product ID**: An exclusive identification number for each product\n",
    "\n",
    "- **Product Review**: Insights and opinions shared by customers about the product\n",
    "\n",
    "- **Sentiment**: Sentiment associated with the product review, indicating whether the review expresses a positive, negative, or neutral sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqfNqaJCQeEE"
   },
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "executionInfo": {
     "elapsed": 22359,
     "status": "ok",
     "timestamp": 1754634341273,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "1Hp4gw7r0EUU",
    "outputId": "cab047fa-8389-466c-eef4-0de432091eb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3.tar.gz (23.3 MB)\n",
      "  Installing build dependencies ...done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "  Installing build dependencies ... done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25done\n",
      "done\n",
      "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1.tar.gz (57.2 MB)\n",
      "  Installing build dependedone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25error\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mPreparing metadata \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[29 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[36m\u001b[1m+ meson setup /private/var/folders/qh/622692_9733gn0sd0ykbtsv40000gn/T/pip-install-_qyew3bf/scipy_61236b9dc43e4d2f80594fcd718b293a /private/var/folders/qh/622692_9733gn0sd0ykbtsv40000gn/T/pip-install-_qyew3bf/scipy_61236b9dc43e4d2f80594fcd718b293a/.mesonpy-aga6zw7f -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=/private/var/folders/qh/622692_9733gn0sd0ykbtsv40000gn/T/pip-install-_qyew3bf/scipy_61236b9dc43e4d2f80594fcd718b293a/.mesonpy-aga6zw7f/meson-python-native-file.ini\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The Meson build system\n",
      "  \u001b[31m   \u001b[0m Version: 1.9.0\n",
      "  \u001b[31m   \u001b[0m Source dir: /private/var/folders/qh/622692_9733gn0sd0ykbtsv40000gn/T/pip-install-_qyew3bf/scipy_61236b9dc43e4d2f80594fcd718b293a\n",
      "  \u001b[31m   \u001b[0m Build dir: /private/var/folders/qh/622692_9733gn0sd0ykbtsv40000gn/T/pip-install-_qyew3bf/scipy_61236b9dc43e4d2f80594fcd718b293a/.mesonpy-aga6zw7f\n",
      "  \u001b[31m   \u001b[0m Build type: native build\n",
      "  \u001b[31m   \u001b[0m Project name: scipy\n",
      "  \u001b[31m   \u001b[0m Project version: 1.13.1\n",
      "  \u001b[31m   \u001b[0m C compiler for the host machine: cc (clang 17.0.0 \"Apple clang version 17.0.0 (clang-1700.0.13.5)\")\n",
      "  \u001b[31m   \u001b[0m C linker for the host machine: cc ld64 1167.5\n",
      "  \u001b[31m   \u001b[0m C++ compiler for the host machine: c++ (clang 17.0.0 \"Apple clang version 17.0.0 (clang-1700.0.13.5)\")\n",
      "  \u001b[31m   \u001b[0m C++ linker for the host machine: c++ ld64 1167.5\n",
      "  \u001b[31m   \u001b[0m Cython compiler for the host machine: cython (cython 3.0.12)\n",
      "  \u001b[31m   \u001b[0m Host machine cpu family: aarch64\n",
      "  \u001b[31m   \u001b[0m Host machine cpu: aarch64\n",
      "  \u001b[31m   \u001b[0m Program python found: YES (/opt/anaconda3/bin/python3.13)\n",
      "  \u001b[31m   \u001b[0m Did not find pkg-config by name 'pkg-config'\n",
      "  \u001b[31m   \u001b[0m Found pkg-config: NO\n",
      "  \u001b[31m   \u001b[0m Run-time dependency python found: YES 3.13\n",
      "  \u001b[31m   \u001b[0m Program cython found: YES (/private/var/folders/qh/622692_9733gn0sd0ykbtsv40000gn/T/pip-build-env-jf_kvak1/overlay/bin/cython)\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-unused-but-set-variable: YES\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-unused-function: YES\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-conversion: YES\n",
      "  \u001b[31m   \u001b[0m Compiler for C supports arguments -Wno-misleading-indentation: YES\n",
      "  \u001b[31m   \u001b[0m Library m found: YES\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m ../meson.build:78:0: ERROR: Compiler gfortran cannot compile programs.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m A full log can be found at /private/var/folders/qh/622692_9733gn0sd0ykbtsv40000gn/T/pip-install-_qyew3bf/scipy_61236b9dc43e4d2f80594fcd718b293a/.mesonpy-aga6zw7f/meson-logs/meson-log.txt\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4414,
     "status": "ok",
     "timestamp": 1754634354359,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "xhZUQ78gzw8H",
    "outputId": "0b9db6ae-6db0-4e47-d708-5dbf665852e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhinavroyce/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abhinavroyce/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# to import Word2Vec\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# to split data into train and test sets\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# to read and manipulate the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('max_colwidth', None)    # setting column to the maximum column width as per the data\n",
    "\n",
    "# to visualise data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# to use regular expressions for manipulating text data\n",
    "import re\n",
    "\n",
    "# to load the natural language toolkit\n",
    "import nltk\n",
    "nltk.download('stopwords')    # loading the stopwords\n",
    "nltk.download('wordnet')    # loading the wordnet module that is used in stemming\n",
    "\n",
    "# to remove common stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# to perform stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# to create Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# to import Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# to split data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# to build a Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# to compute metrics to evaluate the model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# To tune different models\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fantastic-rebel"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24076,
     "status": "ok",
     "timestamp": 1754634380151,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "50euqeNHHNjK",
    "outputId": "cf858cac-823d-484d-a51e-bb51546b056a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CThCS4NJzw8S"
   },
   "outputs": [],
   "source": [
    "# loading data into a pandas dataframe\n",
    "reviews = pd.read_csv(\"/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/Product_Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEvz0b9gzw8U"
   },
   "outputs": [],
   "source": [
    "# creating a copy of the data\n",
    "data = reviews.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvlzvKeqAH-i"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIH4md8nAL4v"
   },
   "source": [
    "### Checking the first five rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1754474544677,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "bj4-QHJ6BiCS",
    "outputId": "219b3c3c-f9ca-4c10-de6e-0fe46af96773"
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuWYF7W_AQx_"
   },
   "source": [
    "### Checking the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1754474549672,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "Mcb3m-xKzw8V",
    "outputId": "16aa4e04-24ca-40e6-87f0-85cc989d5855"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPbc-9HLs2WI"
   },
   "source": [
    "* The dataset has 1007 rows and 3 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBuO6NvsAT1k"
   },
   "source": [
    "### Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754475571550,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "k0XZhWGRBiCV",
    "outputId": "2b17bfde-e1db-422e-edec-06fdd45473a7"
   },
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i5McJGWBiCV"
   },
   "source": [
    "* There are no missing values in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZdNFg-5Zmiz"
   },
   "source": [
    "### Checking for duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1754475575801,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "gn5VDFNoBiCW",
    "outputId": "4ede4418-97f4-4901-ea61-dce430b98f8e"
   },
   "outputs": [],
   "source": [
    "# checking for duplicate values\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwMJutR2BiCW"
   },
   "source": [
    "* There are 2 duplicate values in the dataset.\n",
    "* We'll drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1754634395408,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "3mgEIbCEcurU",
    "outputId": "43aa517a-ba57-4f88-98d6-0756e1bb163a"
   },
   "outputs": [],
   "source": [
    "# dropping duplicate values\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUJ_B5KxhU3D"
   },
   "source": [
    "## Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2Z3MQ9x1a4d"
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=data, x=\"Sentiment\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THSJ8TOk1a_s"
   },
   "outputs": [],
   "source": [
    "data['Sentiment'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqnuCtG7bvHB"
   },
   "source": [
    "- Majority of the reviews are positive (\\~85%), followed by neutral reviews (8%), and then the positive reviews (\\~7%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LGyNkLbDLmEC"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4fvprg6u5fE"
   },
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJDPhhmvvxJ1"
   },
   "source": [
    "### Removing special characters from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGR2c6dCvT-Q"
   },
   "outputs": [],
   "source": [
    "# defining a function to remove special characters\n",
    "def remove_special_characters(text):\n",
    "    # Defining the regex pattern to match non-alphanumeric characters\n",
    "    pattern = '[^A-Za-z0-9]+'\n",
    "\n",
    "    # Finding the specified pattern and replacing non-alphanumeric characters with a blank string\n",
    "    new_text = ''.join(re.sub(pattern, ' ', text))\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQv4x8npctFZ"
   },
   "outputs": [],
   "source": [
    "# Applying the function to remove special characters\n",
    "data['cleaned_text'] = data['Product Review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 70,
     "status": "ok",
     "timestamp": 1754475602500,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "rCxMsKChvT79",
    "outputId": "f41a974b-272f-4c7e-de84-95716e33c211"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3, ['Product Review','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpsuDWLnyJFw"
   },
   "source": [
    "- We can observe that the function removed the special characters and retained the alphabets and numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DftSZK9yQ74"
   },
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLOmMLSLRJT0"
   },
   "outputs": [],
   "source": [
    "# changing the case of the text data to lower case\n",
    "data['cleaned_text'] = data['cleaned_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 68,
     "status": "ok",
     "timestamp": 1754475610080,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "P80CyzprdHH1",
    "outputId": "11415f0b-b316-454f-f667-b5b702856fa6"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3, ['Product Review','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dF_kFCAxyg6L"
   },
   "source": [
    "- We can observe that all the text has now successfully been converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLoWwpxzylZH"
   },
   "source": [
    "### Removing extra whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjJN53m8RWCW"
   },
   "outputs": [],
   "source": [
    "# removing extra whitespaces from the text\n",
    "data['cleaned_text'] = data['cleaned_text'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1754475616439,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "yCx3mBXiHRax",
    "outputId": "2afacd36-9316-4d29-8d1f-f3bb703643dd"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3, ['Product Review','cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwVOVENFz9fJ"
   },
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddYQH3ef0AUj"
   },
   "source": [
    "* The idea with stop word removal is to **exclude words that appear frequently throughout** all the documents in the corpus.\n",
    "* Pronouns and articles are typically categorized as stop words.\n",
    "* The `NLTK` library has an in-built list of stop words and it can utilize that list to remove the stop words from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zof2x5co2X8g"
   },
   "outputs": [],
   "source": [
    "# defining a function to remove stop words using the NLTK library\n",
    "def remove_stopwords(text):\n",
    "    # Split text into separate words\n",
    "    words = text.split()\n",
    "\n",
    "    # Removing English language stopwords\n",
    "    new_text = ' '.join([word for word in words if word not in stopwords.words('english')])\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtmKZqDwfOlt"
   },
   "outputs": [],
   "source": [
    "# Applying the function to remove stop words using the NLTK library\n",
    "data['cleaned_text_without_stopwords'] = data['cleaned_text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 82,
     "status": "ok",
     "timestamp": 1754475635174,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "zikSkmBDfsu9",
    "outputId": "818fe9a2-af42-4b9f-a015-ef36764f3804"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:3,['cleaned_text','cleaned_text_without_stopwords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6T0uCuekb44x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0_cueGLW6Vt"
   },
   "source": [
    "* We observe that all the stopwords have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tN9S84Sj2om2"
   },
   "source": [
    "### Stemming/Lemmatization\n",
    "We will use lemmatization because we got better results using that on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1754635752220,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "fPLoGW5zcATx",
    "outputId": "486cedcd-8566-43b2-c070-77fb874b08e7"
   },
   "outputs": [],
   "source": [
    "# Function to apply lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet #Downloads the WordNet lexical database.WordNet is adictionary-like database where Words are grouped into sets of synonyms\n",
    "import nltk\n",
    "\n",
    "# Make sure to download WordNet resources if not already done\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4') # Open Multilingual WordNet package -This adds language translations, richer word forms, and improved morphological data to WordNet.\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# defining a function to perform stemming\n",
    "def apply_lemmatizer(text):\n",
    "    # Split text into separate words\n",
    "    words = text.split()\n",
    "\n",
    "    # Applying the Porter Stemmer on every word of a message and joining the stemmed words back into a single string\n",
    "    new_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR0rx6_2oCzY"
   },
   "outputs": [],
   "source": [
    "# Applying the function to perform stemming\n",
    "data['final_cleaned_text'] = data['cleaned_text_without_stopwords'].apply(apply_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1754635761104,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "RAc0gMU89KM7",
    "outputId": "1575f798-4113-4126-9f5b-df0ce6a729b3"
   },
   "outputs": [],
   "source": [
    "# checking a couple of instances of cleaned data\n",
    "data.loc[0:2,['cleaned_text_without_stopwords','final_cleaned_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lL6l3xgTLqYP"
   },
   "source": [
    "================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3F-popAzw8p"
   },
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCc_bu4hH-61"
   },
   "source": [
    "### 1. Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm4tWt_FH-62"
   },
   "source": [
    "- We'll use the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class of sklearn to vectorize the data using Bag of Words (BoW).\n",
    "\n",
    "- We first create the document-term matrix, where each value in the matrix stores the count of a term in a document.\n",
    "\n",
    "- We then consider only the top *n* terms by frequency\n",
    "    - *n* is a hyperparameter that one can change and experiment with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1754635813061,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "yxmWAIRrH-62",
    "outputId": "ae66bbac-9740-4c40-bc24-a13f14ce95d7"
   },
   "outputs": [],
   "source": [
    "# Initializing CountVectorizer with top 1000 words\n",
    "bow_vec = CountVectorizer(max_features = 1000)\n",
    "\n",
    "# Applying CountVectorizer on data\n",
    "data_features_BOW = bow_vec.fit_transform(data['final_cleaned_text'])\n",
    "\n",
    "# Convert the data features to array\n",
    "data_features_BOW = data_features_BOW.toarray()\n",
    "\n",
    "\n",
    "# Shape of the feature vector\n",
    "print(\"Shape of the feature vector\",data_features_BOW.shape)\n",
    "\n",
    "# Getting the 1000 words considered by the BoW model\n",
    "words = bow_vec.get_feature_names_out()\n",
    "\n",
    "print(\"first 10 words\",words[:10])\n",
    "print(\"last 10 words\",words[-10:])\n",
    "\n",
    "# Creating a DataFrame from the data features\n",
    "df_BOW = pd.DataFrame(data_features_BOW, columns=bow_vec.get_feature_names_out())\n",
    "df_BOW.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NowSY-UH-64"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nJ1Uz-lH-66"
   },
   "source": [
    "- From the above dataframe, we can observe that the word *yet* is present only once in the third document, and the word *would* is presented twice in the fourth document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsvwxPHP1ZPq"
   },
   "source": [
    "## 2. Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2AR8cBgudHh"
   },
   "source": [
    "\n",
    "Word2Vec is a popular technique to convert words into numerical vectors (i.e., embeddings) so that similar words end up having similar vector representations. It helps machines understand the semantic meaning of words based on their context in sentences.\n",
    "\n",
    "* REAL BENEFIT\n",
    "\n",
    "  * After training on a large corpus, Word2Vec embeddings capture interesting relationships:\n",
    "\n",
    "    * vector(\"king\") - vector(\"man\") + vector(\"woman\") ≈ vector(\"queen\")\n",
    "\n",
    "\n",
    "#### HOW IT WORKS:\n",
    "Word2Vec has two main models:\n",
    "* 1. CBOW (Continuous Bag of Words)- Predicts the target word from surrounding context words.\n",
    "* 2. Skip-gram- Predicts surrounding context words from a target word.\n",
    "\n",
    "\n",
    "\n",
    "#### Summary:\n",
    "* Word2Vec turns words into vectors based on their context.\n",
    "\n",
    "* It helps models understand semantic relationships.\n",
    "\n",
    "* It works best when trained on large text corpora (like Wikipedia, Google News, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-t0EaaQ1giy"
   },
   "source": [
    "### 2.1 CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NniPQC2Kezf"
   },
   "source": [
    "* Example\n",
    "\n",
    "\"The cat sat on the mat\"\n",
    "\n",
    "We'll use a context window of 2 (i.e., two words before and after the target word).\n",
    "* CBOW works opposite of Skip-gram:\n",
    "\n",
    "Instead of predicting context from a word, it predicts the word from its context.\n",
    "\n",
    "| Context (Input Words) | Target (Predicted Word) |\n",
    "| --------------------- | ----------------------- |\n",
    "| \\[\"The\", \"sat\"]       | \"cat\"                   |\n",
    "| \\[\"cat\", \"on\"]        | \"sat\"                   |\n",
    "| \\[\"sat\", \"the\"]       | \"on\"                    |\n",
    "| \\[\"on\", \"mat\"]        | \"the\"                   |\n",
    "\n",
    "* Note: When using a window size of 2, you can include 2 words on either side if available.\n",
    "\n",
    "So, the CBOW model is trained to learn that the center word (\"cat\") is likely when \"The\" and \"sat\" are around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 93,
     "status": "ok",
     "timestamp": 1754636752858,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "Hrn18FKuKi2g",
    "outputId": "4f9a717e-2f49-4ed4-a90c-65612b8f5d75"
   },
   "outputs": [],
   "source": [
    "# Example CBOW\n",
    "# Note-\n",
    "  # sg=0 → model is trained to predict target word from context (CBOW)\n",
    "  # sg=1 → model is trained to predict context words from target (Skip-gram)\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define corpus\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"rug\"],\n",
    "    [\"cats\", \"and\", \"dogs\", \"are\", \"friends\"],\n",
    "    [\"the\", \"puppy\", \"played\", \"with\", \"the\", \"ball\"],\n",
    "    [\"the\", \"kitten\", \"played\", \"with\", \"the\", \"yarn\"]\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# CBOW model (sg=0 for CBOW, sg=1 for skip-gram)\n",
    "cbow_model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=0)\n",
    "\"\"\"\n",
    "PARAMETERS:\n",
    "1. vector_size=10\n",
    "What it means: Number of dimensions in the word vector.\n",
    "Example: \"cat\" → [0.12, -0.56, 0.91, ...] (10 numbers)\n",
    "Tip: Bigger vectors can store more meaning but need more data & computation.\n",
    "\n",
    "2. window=2\n",
    "What it means: How many words before & after the target word are considered context.\n",
    "Example: In \"The cat sat on the mat\",\n",
    "if target = \"sat\", window=2 → context = \"cat\", \"on\", \"the\", \"mat\".\n",
    "Tip:\n",
    "Small window → local grammar relationships\n",
    "Large window → broader semantic relationships\n",
    "\n",
    "3. min_count=1\n",
    "What it means: Minimum word frequency to be included in the vocabulary.\n",
    "Example:\n",
    "min_count=1 → keep all words (good for small datasets)\n",
    "min_count=5 → ignore words that appear fewer than 5 times (good for large datasets).\n",
    "Tip: Helps remove rare, noisy words in big corpora.\n",
    "\n",
    "4. sg=0 or sg=1\n",
    "What it means: Chooses the training algorithm.\n",
    "sg=0 → CBOW (predict target word from context)\n",
    "sg=1 → Skip-gram (predict context words from target)\n",
    "Example:\n",
    "CBOW: \"cat\", \"on\" → \"sat\"\n",
    "Skip-gram: \"sat\" → \"cat\", \"on\"\n",
    "Tip:\n",
    "CBOW is faster & works well with frequent words.\n",
    "Skip-gram is slower but works better with rare words.\n",
    "\n",
    "5. workers\n",
    "What it means: Number of CPU threads to use in training.\n",
    "Word2Vec can process multiple parts of the training data in parallel to speed things up.\n",
    "Example:\n",
    "workers=1 → use only 1 CPU core (slower, but deterministic results)\n",
    "workers=4 → use 4 CPU cores (faster)\n",
    "Tip:\n",
    "On your personal machine, you can set it to the number of cores you have.\n",
    "On Colab / Jupyter with small datasets, it won’t matter much — but for huge corpora, it makes training much faster.\"\"\"\n",
    "\n",
    "# Vector for a word\n",
    "print(\"Vector for 'cat':\")\n",
    "print(cbow_model.wv['cat'])\n",
    "\n",
    "# Similar words to 'cat'\n",
    "print(\"\\nWords similar to 'cat'and the cosine of angles between those vectors:\")\n",
    "print(cbow_model.wv.most_similar('cat'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_O4W-JCJ1rSj"
   },
   "source": [
    "### 2.2 Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUotvZtM22m0"
   },
   "source": [
    "* EXAMPLE:\n",
    "\n",
    "Let's say we have this simple sentence as our training corpus:\n",
    "\n",
    "\"The cat sat on the mat\"\n",
    "\n",
    "Suppose we want to train a Skip-gram model with a context window of 2.\n",
    "We'll break the sentence into word pairs where the target word predicts context words.\n",
    "\n",
    "Skip-gram pairs (target → context):\n",
    "\n",
    "target: \"cat\" → context: \"The\", \"sat\"\n",
    "\n",
    "target: \"sat\" → context: \"cat\", \"on\"\n",
    "\n",
    "target: \"on\" → context: \"sat\", \"the\"\n",
    "\n",
    "target: \"the\" → context: \"on\", \"mat\"\n",
    "\n",
    "* The model learns vector representations (say, 100-dimensional) for each word, so that:\n",
    "\n",
    "  * Words that appear in similar contexts (like \"cat\" and \"dog\" if seen in a bigger dataset) will have similar vectors.\n",
    "\n",
    "  * The distance (cosine similarity) between similar words will be small (close to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1754643271003,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "fNP7mkSK1luD",
    "outputId": "aad118d9-5be3-4997-98af-93ddc7dd5dfc"
   },
   "outputs": [],
   "source": [
    "# Example on skip gram\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Simple corpus\n",
    "sentences = [\n",
    "    [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
    "    [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"rug\"],\n",
    "    [\"cats\", \"and\", \"dogs\", \"are\", \"friends\"],\n",
    "    [\"the\", \"puppy\", \"played\", \"with\", \"the\", \"ball\"],\n",
    "    [\"the\", \"kitten\", \"played\", \"with\", \"the\", \"yarn\"]\n",
    "]\n",
    "\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Get vector for 'cat'\n",
    "print(model.wv['cat'])\n",
    "\n",
    "# Find similar words\n",
    "print(model.wv.most_similar('cat'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L07Dh8MI5L6S"
   },
   "source": [
    "### CBOW vs Skip-gram Summary\n",
    "| Feature    | CBOW                      | Skip-gram             |\n",
    "| ---------- | ------------------------- | --------------------- |\n",
    "| Input      | Surrounding context words | Target word           |\n",
    "| Output     | Predict center word       | Predict context words |\n",
    "| Faster on  | Large datasets            | Small datasets        |\n",
    "| Better for | Frequent words            | Rare words            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukh33scr5FSv"
   },
   "source": [
    "## Now Applying on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M03VkuU8mF-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1 — Tokenize the text\n",
    "sentences = data['final_cleaned_text'].apply(lambda x: x.split())  # assuming text is already cleaned\n",
    "\n",
    "# Step 2 — Train CBOW Model (sg=0)\n",
    "cbow_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,  # length of each word vector\n",
    "    window=3,         # context window size\n",
    "    min_count=5,      # include all words\n",
    "    sg=0,             # CBOW\n",
    "    workers=4         # CPU cores to use\n",
    ")\n",
    "\n",
    "# Step 3 — Train Skip-gram Model (sg=1)\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=3,\n",
    "    min_count=5,\n",
    "    sg=1,             # Skip-gram\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Step 4 — Function to get sentence vectors\n",
    "def get_sentence_vector(model, tokens):\n",
    "    word_vecs = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(word_vecs) == 0:\n",
    "        return np.zeros(model.vector_size)  # handle empty sentences\n",
    "    return np.mean(word_vecs, axis=0)\n",
    "\n",
    "# Step 5 — Apply to dataset\n",
    "\n",
    "# CBOW Vectors\n",
    "data_cbow_vectors = np.array([get_sentence_vector(cbow_model, tokens) for tokens in sentences])\n",
    "# Skip-gram Vectors\n",
    "data_skipgram_vectors = np.array([get_sentence_vector(skipgram_model, tokens) for tokens in sentences])\n",
    "\n",
    "# Step 6 — Convert to DataFrames (optional)\n",
    "df_cbow = pd.DataFrame(data_cbow_vectors)\n",
    "df_skipgram = pd.DataFrame(data_skipgram_vectors)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1754649567496,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "BHlbCx1Q9li3",
    "outputId": "f1aa0eec-7bfb-4280-c3d7-fd88cfd5a712"
   },
   "outputs": [],
   "source": [
    "# Checking top 5 similar words to the word 'book'\n",
    "similar = cbow_model.wv.similar_by_word('book', topn=5)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1754643573913,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "d_MzWV27t9eZ",
    "outputId": "ebeddb02-4b78-462c-f46a-af389168a041"
   },
   "outputs": [],
   "source": [
    "# Checking top 5 similar words to the word 'review'\n",
    "similar = model_W2V.wv.similar_by_word('review', topn=5)\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jlSTXxIVWBE"
   },
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9K9uNuxSYIVF"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = '/content/drive/MyDrive/0- July-Dec 2025/5th sem Intro to LLM and GenAI/Classroom Mini Projects/Part-2/glove.6B.100d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QykT8OpMS64g"
   },
   "outputs": [],
   "source": [
    "# Checking the word embedding of a random word\n",
    "word = \"book\"\n",
    "model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1754649737430,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "pjMpBimUaXbf",
    "outputId": "05201f64-4ffd-459c-d65d-37e9700b640c"
   },
   "outputs": [],
   "source": [
    "#Returning the top 5 similar words.\n",
    "result = model.most_similar(\"book\", topn=5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1754649740947,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "xqfi6C9BHgpT",
    "outputId": "a446e247-ff26-4229-a0eb-b1d1fef45c2c"
   },
   "outputs": [],
   "source": [
    "#Returning the top 5 similar words.\n",
    "result = model.most_similar(\"review\", topn=5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "executionInfo": {
     "elapsed": 16709,
     "status": "ok",
     "timestamp": 1754650128734,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "vknMh_YjdVX6",
    "outputId": "1b89d867-73ba-442d-eb07-f6b72e607c04"
   },
   "outputs": [],
   "source": [
    "#List of words in the vocabulary\n",
    "words = model.index_to_key\n",
    "\n",
    "#Dictionary with key as the word and the value as the corresponding embedding vector.\n",
    "word_vector_dict = dict(zip(model.index_to_key,list(model.vectors)))\n",
    "\n",
    "#Defining the dimension of the embedded vector.\n",
    "vec_size=100\n",
    "\n",
    "def average_vectorizer_GloVe(doc):\n",
    "    # Initializing a feature vector for the sentence\n",
    "    feature_vector = np.zeros((vec_size,), dtype=\"float64\")\n",
    "\n",
    "    # Creating a list of words in the sentence that are present in the model vocabulary\n",
    "    words_in_vocab = [word for word in doc.split() if word in words]\n",
    "\n",
    "    # adding the vector representations of the words\n",
    "    for word in words_in_vocab:\n",
    "        feature_vector += np.array(word_vector_dict[word])\n",
    "\n",
    "    # Dividing by the number of words to get the average vector\n",
    "    if len(words_in_vocab) != 0:\n",
    "        feature_vector /= len(words_in_vocab)\n",
    "\n",
    "    return feature_vector\n",
    "\n",
    "    # creating a dataframe of the vectorized documents\n",
    "df_glove = pd.DataFrame(data['final_cleaned_text'].apply(average_vectorizer_GloVe).tolist(), columns=['Feature '+str(i) for i in range(vec_size)])\n",
    "df_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45695,
     "status": "ok",
     "timestamp": 1754651657761,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "p8wyfRwywZoR",
    "outputId": "e92a0533-5c88-4ed1-cff2-27e2fbc30c40"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Create a list of datasets and their labels\n",
    "vectorized_datasets = [\n",
    "    (\"BoW\", df_BOW),\n",
    "     (\"GloVe\", df_glove),\n",
    "    (\"word2Vec_cbow\",df_cbow),\n",
    "    (\"skipgram\",df_skipgram)\n",
    "]\n",
    "\n",
    "# Your target variable\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop over each dataset and train both classifiers\n",
    "for name, X in vectorized_datasets:\n",
    "    # Split data (80/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\n",
    "\n",
    "    # Random Forest\n",
    "    rf_model = RandomForestClassifier(random_state=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_f1 = f1_score(y_test, rf_preds, average='macro')\n",
    "    results.append((f\"RandomForest - {name}\", rf_f1, rf_model, X_test, y_test, rf_preds))\n",
    "\n",
    "    \"\"\"# Multinomial Naive Bayes\n",
    "    nb_model = MultinomialNB()\n",
    "    nb_model.fit(X_train, y_train)\n",
    "    nb_preds = nb_model.predict(X_test)\n",
    "    nb_f1 = f1_score(y_test, nb_preds, average='macro')\n",
    "    results.append((f\"NaiveBayes - {name}\", nb_f1, nb_model, X_test, y_test, nb_preds))\"\"\"\n",
    "\n",
    "    # Gradient Boosting\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    gboost = GradientBoostingClassifier(random_state=100)\n",
    "    gboost.fit(X_train, y_train)\n",
    "    gb_preds = gboost.predict(X_test)\n",
    "    gb_f1 = f1_score(y_test, gb_preds, average='macro')\n",
    "    results.append((f\"Gradient Boost - {name}\", gb_f1, gboost, X_test, y_test, gb_preds))\n",
    "\n",
    "    # Ada Boosting\n",
    "    from sklearn.ensemble import AdaBoostClassifier\n",
    "    ada = AdaBoostClassifier()\n",
    "    ada.fit(X_train, y_train)\n",
    "    ada_preds = ada.predict(X_test)\n",
    "    ada_f1 = f1_score(y_test, ada_preds, average='macro')\n",
    "    results.append((f\"Adaptive Boost - {name}\", ada_f1, ada, X_test, y_test, ada_preds))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sort results by F1 score (descending)\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print all F1 scores\n",
    "print(\"\\n📊 Model Performance (Macro F1-scores):\\n\")\n",
    "for label, f1_score_val, _, _, _, _ in results:\n",
    "    print(f\"{label:30s}: Macro F1 = {f1_score_val:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltBcjQywYk2v"
   },
   "source": [
    "100, 3\n",
    "\n",
    "📊 Model Performance (Macro F1-scores):\n",
    "\n",
    "Gradient Boost - BoW          : Macro F1 = 0.6442\n",
    "Gradient Boost - skipgram     : Macro F1 = 0.5362\n",
    "RandomForest - skipgram       : Macro F1 = 0.5354\n",
    "RandomForest - GloVe          : Macro F1 = 0.5138\n",
    "Gradient Boost - word2Vec_cbow: Macro F1 = 0.5096\n",
    "RandomForest - BoW            : Macro F1 = 0.4818\n",
    "RandomForest - word2Vec_cbow  : Macro F1 = 0.4818\n",
    "Gradient Boost - GloVe        : Macro F1 = 0.4776\n",
    "Adaptive Boost - GloVe        : Macro F1 = 0.3715\n",
    "Adaptive Boost - skipgram     : Macro F1 = 0.3436\n",
    "Adaptive Boost - word2Vec_cbow: Macro F1 = 0.3414\n",
    "Adaptive Boost - BoW          : Macro F1 = 0.3025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHheVsGoX2o_"
   },
   "source": [
    "1000, 2\n",
    "\n",
    "📊 Model Performance (Macro F1-scores):\n",
    "\n",
    "Gradient Boost - BoW          : Macro F1 = 0.6442\n",
    "RandomForest - GloVe          : Macro F1 = 0.5138\n",
    "Gradient Boost - skipgram     : Macro F1 = 0.5111\n",
    "RandomForest - BoW            : Macro F1 = 0.4818\n",
    "RandomForest - word2Vec_cbow  : Macro F1 = 0.4818\n",
    "Gradient Boost - GloVe        : Macro F1 = 0.4776\n",
    "RandomForest - skipgram       : Macro F1 = 0.4724\n",
    "Gradient Boost - word2Vec_cbow: Macro F1 = 0.4346\n",
    "Adaptive Boost - GloVe        : Macro F1 = 0.3715\n",
    "Adaptive Boost - skipgram     : Macro F1 = 0.3700\n",
    "Adaptive Boost - BoW          : Macro F1 = 0.3025\n",
    "Adaptive Boost - word2Vec_cbow: Macro F1 = 0.3005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1754643124837,
     "user": {
      "displayName": "Mr. Ashwani Balyan (SU Training Delivery Manager)",
      "userId": "10495710902595717956"
     },
     "user_tz": -330
    },
    "id": "wUzFXh8D4TA7",
    "outputId": "4306b758-558d-46e0-e65a-8cabfd82a0c7"
   },
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model_label, best_f1, best_model, X_test_best, y_test_best, y_pred_best = results[0]\n",
    "\n",
    "print(f\"\\n✅ Best Model: {best_model_label} (Macro F1 = {best_f1:.4f})\\n\")\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test_best, y_pred_best))\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test_best, y_pred_best, labels=best_model.classes_)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=best_model.classes_, yticklabels=best_model.classes_)\n",
    "plt.title(f\"Confusion Matrix: {best_model_label}\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH77U4ukBiCe"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aetabA3poIq"
   },
   "source": [
    "- We analyzed the distribution of sentiments of the customers.\n",
    "\n",
    "- We used different text processing techniques to clean the raw text data.\n",
    "\n",
    "- We then built an ML model (Random Forest) with the vectorized data.\n",
    "\n",
    "- The Random Forest model was able to achieve a recall score of 88% on the test dataset.\n",
    "    - The model can be tuned further or a different model can be trained to model the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEXtBjG_oYwj"
   },
   "source": [
    "- By pinpointing areas of improvement or concerns raised by customers based on the predictions of the model, the organization can take swift and targeted actions to address issues, minimizing the risk of revenue loss and bolstering customer satisfaction.\n",
    "\n",
    "- The organization can leverage sentiment categorizations to tailor marketing strategies.\n",
    "    - Highlighting positive sentiments in promotional material can contribute to a positive brand image.\n",
    "    - They can use neutral and negative sentiments to make informed decisions around inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2Lw8akCLT9c"
   },
   "source": [
    "<font size=6 color='blue'>Thanks.....</font>\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MjncuDf2qugI",
    "x9VSf2D_F5iU",
    "lJ6qUJ3EqxW2",
    "saFx1pbT_zTP",
    "tqfNqaJCQeEE",
    "fantastic-rebel",
    "vvlzvKeqAH-i",
    "cIH4md8nAL4v",
    "NuWYF7W_AQx_",
    "EBuO6NvsAT1k",
    "nZdNFg-5Zmiz",
    "kUJ_B5KxhU3D",
    "akIOIRhfbvG8",
    "N4fvprg6u5fE",
    "NJDPhhmvvxJ1",
    "2DftSZK9yQ74",
    "hLoWwpxzylZH",
    "EwVOVENFz9fJ",
    "tN9S84Sj2om2",
    "KCc_bu4hH-61",
    "Q2AR8cBgudHh",
    "rkFhtFGeeIj0",
    "VqiWTT5PhcR8",
    "f4lwYN5bYmHp",
    "6-buwKTziQFM",
    "0tdJTAgSLZnJ",
    "Kbp4IDZTLcDW",
    "3t3QwyS6YsoC",
    "9SGdP8lvLeRX",
    "R1LUc380Lec0",
    "beneficial-bosnia",
    "LX_Y_iceLf6F",
    "1XPSU85ELgCS",
    "J1P1b0bnmfa6",
    "wH77U4ukBiCe"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
